<!DOCTYPE html><html prefix="og: http://ogp.me/ns#"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title>python小说爬虫 · Let's go for a walk around the world - zuo</title><meta name="description" content="python小说爬虫 - John Doe"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.ico"><link rel="stylesheet" href="//at.alicdn.com/t/font_1472710214_6648843.css"><link rel="stylesheet" href="//raw.githack.com/xwartz/hexo-theme-nuna/next/source/style/main.css?v=1.0.3"></head><body class="pupa"><div class="loading-bar"></div><main><div class="post post"><article itemscope itemtype="http://schema.org/Article" class="hentry"><div class="container"><div class="entry-header"><h1 class="entry-title">python小说爬虫</h1><div class="entry-description"></div><div class="entry-meta"><time itemprop="datePublished" datetime="Wednesday, April 25th 2018, 8:30:00 pm" class="updated">Apr 25, 2018</time><em class="post-count">1,734 words</em></div></div><div itemprop="articleBody" class="entry-content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>很久没有写爬虫了，最近接到一个抓取小说的项目顺便做此纪录练练手，之后工作中可能也会有部分场景要用到爬虫，爬取竞争对手进行数据分析什么的。<br>目标网站：<a href="http://www.xxsy.net/" target="_blank" rel="external"><strong>潇湘书院</strong></a><br>环境准备：</p>
<ul>
<li>python3</li>
<li>requests库</li>
<li>BeautifulSoup库</li>
</ul>
<h2 id="整体思路"><a href="#整体思路" class="headerlink" title="整体思路"></a>整体思路</h2><p>抓取这个小说网站免费板块的所有内容，查看页面发现这个板块一共有6697页，每页有20本小说，那整体思路就是先抓取每页的20个小说名称和url，然后进入每本小说的阅读地址，拿到每一个章节的标题和url，抓取每章节的正文内容并写到本地txt文本中。<br><img src="123" alt=""></p>
<h2 id="单页分析"><a href="#单页分析" class="headerlink" title="单页分析"></a>单页分析</h2><p>这里请求使用requests，解析页面用非常方便的BeautifulSoup，在一个文章标题上右键检查，在高亮的这条a标签右键，copy selector，通过这条selector来定位BeautifulSoup解析后小说所在的位置。<br><img src="http://otlbf411d.bkt.clouddn.com/18-4-26/95059112.jpg" alt=""><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">body &gt; div.content &gt; div &gt; div &gt; div.inner-mainbar &gt; div.search-result &gt; div.result-list &gt; ul &gt; li:nth-child(1) &gt; div &gt; h4 &gt; a</div></pre></td></tr></table></figure></p>
<p>其中<code>li:nth-child(1)</code>很明显是指这个小说在当前页面小说列表中排列第一个，我们想要本页面所有的20本小说，所以就删掉这个<code>:nth-child(1)</code>，再将selector语句精简一下只要能定位到即可，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">div.result-list &gt; ul &gt; li &gt; div &gt; h4 &gt; a</div></pre></td></tr></table></figure></p>
<p>然后将它写入代码，print查看一下结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests,os</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line">webdata = requests.get(<span class="string">'http://www.xxsy.net/search?vip=0&amp;sort=2'</span>)</div><div class="line">soup = BeautifulSoup(webdata.text,<span class="string">'lxml'</span>)</div><div class="line">books = soup.select(<span class="string">'div.result-list &gt; ul &gt; li &gt; div &gt; h4 &gt; a'</span>)</div><div class="line">print(books)</div></pre></td></tr></table></figure>
<p><img src="http://otlbf411d.bkt.clouddn.com/18-4-26/26352205.jpg" alt=""><br>结果是一个列表，列表中有20个元素，分别对应的20个小说，我们想要的是每个元素中href后面的链接和小说的名字，用循环提取出来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line"><span class="keyword">for</span> book <span class="keyword">in</span> books:</div><div class="line">    bookname = book.text</div><div class="line">    bookurl = <span class="string">'http://www.xxsy.net'</span> + book.get(<span class="string">'href'</span>)</div><div class="line">    bookid = book.get(<span class="string">'href'</span>).split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'.'</span>)[<span class="number">0</span>]</div><div class="line">    print(bookname,bookurl,bookid)</div></pre></td></tr></table></figure>
<p><img src="http://otlbf411d.bkt.clouddn.com/18-4-26/86095250.jpg" alt=""><br>这样就很简单的拿到了单个页面20本小说的标题和url地址，可以看到上面我把地址中的一串数字用单独取出来了，这个数字其实就是小说对应的唯一ID，后面要用到。</p>
<h2 id="章节分析"><a href="#章节分析" class="headerlink" title="章节分析"></a>章节分析</h2><p>点击一个小说进入详情页面，切换至”作品目录”下可以看到所有的章节，这时候在一个章节上右键检查，跟上面方法一样，同样可以很简单的获取到该小说所有章节的名称和url地址以及章节ID。<br><img src="http://otlbf411d.bkt.clouddn.com/18-4-26/51873134.jpg" alt=""></p>
<p>为了在一篇文章介绍尽可能多的方法，这里我不用上面这个地方来获取章节信息，从另一个地方进入。点击“开始阅读”进入第一章节的正文，在页面的左边可以看到一个“目录”按钮，点看以后就能看到所有的章节名称了。然而在这些章节名称上右键发现当前页面禁用了鼠标右键功能，无法检查元素当然也就不能copy它的selector。<br><img src="http://otlbf411d.bkt.clouddn.com/18-4-26/63797573.jpg" alt=""></p>
<p>这种按钮的点击肯定是向服务器发送了请求的，打开fiddler进行抓包，再次点击目录按钮，此时可以看到这个请求已经被成功捕获，点击这条请求查看详细信息，这个接口的功能就是查询所有的章节信息。<br><img src="http://otlbf411d.bkt.clouddn.com/18-4-26/17982944.jpg" alt=""></p>
<p>真实的请求地址是<code>http://www.xxsy.net/partview/GetChapterListNoSub?bookid=945608&amp;isvip=0</code>，其中bookid就是我们前面找到的bookid，这里可以做成参数化依次传入其他小说的ID。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line">url = <span class="string">'http://www.xxsy.net/partview/GetChapterListNoSub?bookid=945608&amp;isvip=0'</span></div><div class="line">titles = BeautifulSoup(requests.get(url).text,<span class="string">'lxml'</span>).select(<span class="string">'ul &gt; li &gt; a'</span>)</div><div class="line"><span class="keyword">for</span> title <span class="keyword">in</span> titles:</div><div class="line">    titlename = title.text  <span class="comment">#章节名称</span></div><div class="line">    titleurl = <span class="string">'http://www.xxsy.net'</span> + title.get(<span class="string">'href'</span>)   <span class="comment">#章节地址</span></div><div class="line">    print(titlename,titleurl)</div></pre></td></tr></table></figure></p>
<p><img src="http://otlbf411d.bkt.clouddn.com/18-4-26/53687066.jpg" alt=""></p>
<h2 id="正文下载"><a href="#正文下载" class="headerlink" title="正文下载"></a>正文下载</h2><p>上一步取到的titleurl是每一章节的阅读地址，直接requests请求并解析拿到正文内容<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">contents = BeautifulSoup(requests.get(titleurl).text, <span class="string">'lxml'</span>).select(<span class="string">'div#auto-chapter &gt; p'</span>)   <span class="comment">#正文内容</span></div></pre></td></tr></table></figure></p>
<p><img src="http://otlbf411d.bkt.clouddn.com/18-4-26/83734461.jpg" alt=""><br>返回结果是一个列表，每一个元素是一个段落，将每一段内容前后无用的标签剔除并写到本地的txt文本，写入方式为<code>a+</code>,每次写入时在后面追加，不会覆盖之前的内容，文本自动按照前面获取到的小说名来命名。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line"><span class="keyword">for</span> content <span class="keyword">in</span> contents:</div><div class="line">    content = str(content).replace(<span class="string">'&lt;p&gt;'</span>, <span class="string">'\n'</span>).replace(<span class="string">'&lt;/p&gt;'</span>, <span class="string">''</span>)</div><div class="line">    f = open(path + <span class="string">'%s.txt'</span> % bookname, <span class="string">'a+'</span>)</div><div class="line">    f.write(content)</div><div class="line">    f.close()</div></pre></td></tr></table></figure></p>
<p><img src="http://otlbf411d.bkt.clouddn.com/18-4-26/31951196.jpg" alt=""></p>
<p>到此为止三个步骤已经完成，可以顺利的爬下一个章节的内容了，现在通过几个for循环将这几个步骤整合，就可以源源不断的开始下载小说了。<br><img src="http://otlbf411d.bkt.clouddn.com/18-4-26/272032.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding=utf-8</span></div><div class="line"><span class="keyword">import</span> requests,time,os</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line">path = <span class="string">r'C:\Users\lipei\Desktop\潇湘书院爬虫\小说\\'</span>               <span class="comment">#本地存放小说的路径</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_books</span><span class="params">(url)</span>:</span></div><div class="line">    webdata = requests.get(url,timeout=<span class="number">60</span>)   </div><div class="line">    soup = BeautifulSoup(webdata.text,<span class="string">'lxml'</span>)</div><div class="line">    books = soup.select(<span class="string">'div.result-list &gt; ul &gt; li &gt; div &gt; h4 &gt; a'</span>)</div><div class="line">    <span class="keyword">for</span> book <span class="keyword">in</span> books:</div><div class="line">        bookid = book.get(<span class="string">'href'</span>).split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'.'</span>)[<span class="number">0</span>]   <span class="comment">#小说ID作为下一个请求ur中的参数</span></div><div class="line">        bookname = book.text                                     <span class="comment">#小说名称</span></div><div class="line">        bookurl = <span class="string">'http://www.xxsy.net'</span> + book.get(<span class="string">'href'</span>)       <span class="comment">#小说url地址</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> bookname+<span class="string">'.txt'</span> <span class="keyword">in</span> oldlists:                          <span class="comment">#判断是否已经下载过，若存在则跳过</span></div><div class="line">            <span class="keyword">continue</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">pass</span></div><div class="line"></div><div class="line">        print(<span class="string">'=====================正在下载【'</span> + bookname + <span class="string">'】====================='</span>)</div><div class="line">        url = <span class="string">'http://www.xxsy.net/partview/GetChapterListNoSub?bookid=%s&amp;isvip=0'</span> % bookid</div><div class="line">        titles = BeautifulSoup(requests.get(url,timeout=<span class="number">60</span>).text,<span class="string">'lxml'</span>).select(<span class="string">'ul &gt; li &gt; a'</span>)</div><div class="line"></div><div class="line">        <span class="keyword">for</span> title <span class="keyword">in</span> titles:</div><div class="line">            titleurl = <span class="string">'http://www.xxsy.net'</span> + title.get(<span class="string">'href'</span>)  <span class="comment">#章节url地址</span></div><div class="line">            titlename = title.text                                <span class="comment">#章节名称</span></div><div class="line">            <span class="comment"># print(titlename,titleurl)</span></div><div class="line">            <span class="keyword">try</span>:</div><div class="line">                f = open(path + <span class="string">'%s.txt'</span> % bookname, <span class="string">'a+'</span>)        <span class="comment">#章节名称写到txt文本</span></div><div class="line">                f.write(<span class="string">'\n'</span>*<span class="number">2</span> + titlename + <span class="string">'\n'</span>)</div><div class="line">                f.close()</div><div class="line">            <span class="keyword">except</span>:</div><div class="line">                <span class="keyword">pass</span></div><div class="line"></div><div class="line">            contents = BeautifulSoup(requests.get(titleurl,timeout=<span class="number">60</span>).text, <span class="string">'lxml'</span>).select(<span class="string">'div#auto-chapter &gt; p'</span>)</div><div class="line">            <span class="keyword">for</span> content <span class="keyword">in</span> contents:</div><div class="line">                content = str(content).replace(<span class="string">'&lt;p&gt;'</span>, <span class="string">'\n'</span>).replace(<span class="string">'&lt;/p&gt;'</span>, <span class="string">''</span>)</div><div class="line">                <span class="keyword">try</span>:</div><div class="line">                    f = open(path + <span class="string">'%s.txt'</span> % bookname, <span class="string">'a+'</span>)   <span class="comment">#正文内容卸载txt文本，紧接在章节名称的下面</span></div><div class="line">                    f.write(content)</div><div class="line">                    f.close()</div><div class="line">                <span class="keyword">except</span>:</div><div class="line">                    <span class="keyword">pass</span></div><div class="line">            print(titlename + <span class="string">'[已下载]'</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    urls = [<span class="string">'http://www.xxsy.net/search?vip=0&amp;sort=2&amp;pn=&#123;&#125;'</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6697</span>)]   <span class="comment">#免费板块每一页的url通过最后的pn参数控制</span></div><div class="line">    <span class="keyword">global</span> oldlists</div><div class="line">    oldlists = os.listdir(path)        <span class="comment">#爬虫开始之前检查当前目录已有的文件</span></div><div class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> urls:</div><div class="line">        get_books(url)</div><div class="line"></div><div class="line">        <span class="comment">#控制每天爬取的数量，达到要求后停止任务，通过任务开始前后目录中的文件数量相减来判断</span></div><div class="line">        newlists = os.listdir(path)</div><div class="line">        num = len(newlists) - len(oldlists)</div><div class="line">        <span class="keyword">if</span> num &gt;= <span class="number">20</span>:</div><div class="line">            print(<span class="string">'今日任务下载完毕，今日下载小说%d本'</span> % num)</div><div class="line">            <span class="keyword">break</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>在测试过程中发现短时间持续请求该网站的话会可能被服务器拒绝，但是并没有封禁IP，这可能是这个网站唯一的反爬措施了，然而并没有卵用，将请求放在一个无限循环里面，若被拒绝就自动重连，连上以后跳出循环。<br>后面如果遇到封禁IP的网站再讲如何通过更换IP来规避。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        webdata = requests.get(url,timeout=<span class="number">60</span>)</div><div class="line">        <span class="keyword">break</span>      <span class="comment">#连接成功就跳出循环</span></div><div class="line">    <span class="keyword">except</span>:</div><div class="line">        time.sleep(<span class="number">3</span>)</div></pre></td></tr></table></figure></p>
<font color="white">xx</font>
</div><div class="entry-extra"><div class="entry-tags"><a href="/tags/python/" class="tag">python</a><a href="/tags/爬虫/" class="tag">爬虫</a></div></div></div></article></div></main><footer><div class="copyright container"><p>© Copyright 2018 by <a href="http://yoursite.com">John Doe</a>.</p></div></footer><script async src="//cdn.bootcss.com/mathjax/2.7.0-beta.0/MathJax.js"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-69822347-1",'auto');
ga('set', 'appName',"Let's go for a walk around the world - zuo");
ga('send','pageview');</script><script>(function () { var sid =500303098;cid =500303108;var hm = document.createElement('script');
hm.src = 'http://pingjs.qq.com/h5/stats.js';
hm.setAttribute('name', 'MTAH5'); hm.setAttribute('sid', sid); hm.setAttribute('cid', cid);
var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);
}())</script><script>var _vds = _vds || [];
window._vds = _vds;
(function(){
  ;_vds.push(['setAccountId',"90b580e047dd0007"  ]);
  (function() {
      var vds = document.createElement('script');
      vds.type='text/javascript';
      vds.async = true;
      vds.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'dn-growing.qbox.me/vds.js';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(vds, s);
  })();
})();</script><script async src="//raw.githack.com/Easyfood/pageAccelerator/master/dist/page-accelerator.min.js"></script><script async src="/script/loading.js"></script><script async src="/script/photo.js"></script></body></html>